{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Pcc9QMzFVbE"
      },
      "outputs": [],
      "source": [
        "#@title RAG ë°ì´í„° í™•ì¸ìš©_ê²€ìƒ‰ ìˆœìœ„ í‰ê°€ ë„êµ¬\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1. ì„¤ì¹˜ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "# ------------------------------------------------\n",
        "!pip install -q openai numpy scipy ipywidgets\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import numpy as np\n",
        "import openai\n",
        "from scipy.spatial.distance import cosine\n",
        "import ipywidgets as widgets\n",
        "from google.colab import files\n",
        "from google.colab import userdata\n",
        "from IPython.display import display, clear_output\n",
        "import re\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2. API í‚¤ ì„¤ì • ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
        "# ------------------------------------------------\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    if not OPENAI_API_KEY:\n",
        "        raise ValueError(\"API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. Colabì˜ ë¹„ë°€ ê¸°ëŠ¥ìœ¼ë¡œ 'OPENAI_API_KEY'ë¥¼ ì„¤ì •í•˜ì‹­ì‹œì˜¤.\")\n",
        "    openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "    print(\"âœ… OpenAI API í‚¤ ë¡œë“œ ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3. ì „ì—­ ë³€ìˆ˜ (ì‘ìš© í”„ë¡œê·¸ë¨ ìƒíƒœ ê´€ë¦¬)\n",
        "# ------------------------------------------------\n",
        "documents = [] # í˜•ì‹: [{\"id\": int, \"text\": str, \"metadata\": dict}, ...]\n",
        "doc_embeddings = None\n",
        "last_embedding_model = None\n",
        "last_division_settings = None\n",
        "file_name = \"uploaded_data\"\n",
        "current_query = \"\"\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4. UI êµ¬ì„± ìš”ì†Œ ì •ì˜\n",
        "# ------------------------------------------------\n",
        "file_uploader = widgets.FileUpload(accept='.txt,.json', multiple=False, description='íŒŒì¼ ì—…ë¡œë“œ')\n",
        "division_method_selector = widgets.Dropdown(\n",
        "    options=[('1í–‰ì„ 1ë¬¸ì„œë¡œ ë¶„í• ', 'line'), ('ê³ ì • ê¸¸ì´ì˜ ì²­í¬ë¡œ ë¶„í• ', 'chunk'), ('JSON ê°ì²´ë³„ë¡œ ë¶„í• ', 'json_object')],\n",
        "    value='line', description='ë¶„í•  ë°©ë²•:', style={'description_width': 'initial'}\n",
        ")\n",
        "json_content_key_input = widgets.Text(value='content', placeholder='ì½˜í…ì¸ ì˜ í‚¤ ì´ë¦„', description='JSON ì½˜í…ì¸  í‚¤:', layout=widgets.Layout(display='none'))\n",
        "chunk_size_input = widgets.IntText(value=500, description='ì²­í¬ í¬ê¸°:', layout=widgets.Layout(display='none', width='200px'))\n",
        "chunk_overlap_input = widgets.IntText(value=50, description='ì˜¤ë²„ë©:', layout=widgets.Layout(display='none', width='200px'))\n",
        "division_settings_box = widgets.HBox([chunk_size_input, chunk_overlap_input, json_content_key_input])\n",
        "embedding_model_selector = widgets.Dropdown(\n",
        "    options=[('ì‚¬ìš© ì•ˆ í•¨(í‚¤ì›Œë“œ ê²€ìƒ‰)', 'none'), ('text-embedding-3-small', 'text-embedding-3-small'), ('text-embedding-3-large', 'text-embedding-3-large'), ('text-embedding-ada-002', 'text-embedding-ada-002')],\n",
        "    value='text-embedding-3-small', description='ë‚´ì¥ ëª¨ë¸:', style={'description_width': 'initial'}\n",
        ")\n",
        "query_input = widgets.Text(value='', placeholder='ê²€ìƒ‰ ì¿¼ë¦¬ ì…ë ¥...', description='ì¿¼ë¦¬:', layout=widgets.Layout(width='80%'))\n",
        "search_button = widgets.Button(description='ê²€ìƒ‰ì‹¤í–‰', button_style='success', icon='search')\n",
        "output_area = widgets.Output()\n",
        "download_area = widgets.Output()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5. ë„ìš°ë¯¸ í•¨ìˆ˜ (ì½”ì–´ ë¡œì§)\n",
        "# ------------------------------------------------\n",
        "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
        "    if not text or not isinstance(text, str): return None\n",
        "    try:\n",
        "        return openai_client.embeddings.create(input=[text.replace(\"\\n\", \" \")], model=model).data[0].embedding\n",
        "    except Exception as e:\n",
        "        with output_area: print(f\"ë‚´ì¥ ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
        "        return None\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size, chunk_overlap):\n",
        "    if chunk_size <= chunk_overlap: raise ValueError(\"ì²­í¬ í¬ê¸°ëŠ” ì˜¤ë²„ë©ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.\")\n",
        "    chunks, start_index = [], 0\n",
        "    while start_index < len(text):\n",
        "        end_index = start_index + chunk_size\n",
        "        chunks.append(text[start_index:end_index])\n",
        "        start_index += chunk_size - chunk_overlap\n",
        "    return chunks\n",
        "\n",
        "def update_documents(new_docs):\n",
        "    global documents, doc_embeddings, last_division_settings\n",
        "    documents = new_docs\n",
        "    doc_embeddings, last_division_settings = None, None\n",
        "    with output_area: print(f\"âœ… {len(documents)} ë¬¸ì„œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\")\n",
        "    display_source_document_downloader()\n",
        "\n",
        "def display_source_document_downloader():\n",
        "    with download_area:\n",
        "        clear_output()\n",
        "        if documents:\n",
        "            button = widgets.Button(description=f\"ë¶„í• ëœ ì „ì²´{len(documents)}ê°œì˜ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ\", button_style='info', icon='download')\n",
        "            def download_source(b):\n",
        "                base_name = os.path.splitext(file_name)[0]\n",
        "                dl_filename = f\"divided_{base_name}.txt\"\n",
        "                content = \"\\n\".join([f\"--- ë¶„í•  ë¬¸ì„œ ID: {d['id']} ---\\n{d.get('text', '')}\\n\" for d in documents])\n",
        "                with open(dl_filename, \"w\", encoding=\"utf-8\") as f: f.write(content)\n",
        "                files.download(dl_filename)\n",
        "            button.on_click(download_source)\n",
        "            display(widgets.VBox([widgets.HTML(\"<hr>\"), button]))\n",
        "\n",
        "def display_results_downloader(results, query):\n",
        "    \"\"\"â˜…ë³µì›â˜…: ê²€ìƒ‰ ê²°ê³¼ë¥¼ CSVë¡œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ë²„íŠ¼ í‘œì‹œ\"\"\"\n",
        "    with download_area:\n",
        "        clear_output(wait=True)\n",
        "        display_source_document_downloader()\n",
        "        if results:\n",
        "            button = widgets.Button(description=f\"ëª¨ë“  {len(results)}ê°œì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ CSVì—ì„œ ë‹¤ìš´ë¡œë“œ\", button_style='success', icon='download')\n",
        "            def download_csv(b):\n",
        "                query_sanitized = re.sub(r'[\\\\/*?:\"<>|]', \"\", query)[:20]\n",
        "                dl_filename = f\"search_results_{query_sanitized}.csv\"\n",
        "                with open(dl_filename, 'w', newline='', encoding='utf-8-sig') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow(['Rank', 'Split_ID', 'Score', 'Text'])\n",
        "                    for i, r in enumerate(results):\n",
        "                        score_str = f\"{r['score']:.6f}\" if isinstance(r['score'], float) else str(r['score'])\n",
        "                        writer.writerow([i + 1, r['doc_info']['id'], score_str, r['doc_info']['text']])\n",
        "                files.download(dl_filename)\n",
        "            button.on_click(download_csv)\n",
        "            display(button)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 6. ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ (UI ë™ì‘ ì •ì˜)\n",
        "# ------------------------------------------------\n",
        "def on_division_method_change(change):\n",
        "    method = change['new']\n",
        "    chunk_size_input.layout.display = 'flex' if method == 'chunk' else 'none'\n",
        "    chunk_overlap_input.layout.display = 'flex' if method == 'chunk' else 'none'\n",
        "    json_content_key_input.layout.display = 'flex' if method == 'json_object' else 'none'\n",
        "\n",
        "def on_file_upload(change):\n",
        "    # (ì´ í•¨ìˆ˜ëŠ” ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤)\n",
        "    global file_name\n",
        "    uploaded_file = change['new']\n",
        "    if not uploaded_file: return\n",
        "    file_info = next(iter(uploaded_file.values()))\n",
        "    file_name, content_bytes = file_info['metadata']['name'], file_info['content']\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        print(f\"'{file_name}' ë¡œë“œ ì¤‘...\")\n",
        "        try:\n",
        "            new_docs = []\n",
        "            method = division_method_selector.value\n",
        "            if method == 'json_object':\n",
        "                if not file_name.endswith('.json'): raise ValueError(\"ì´ ë¶„í•  ë°©ë²•ì€ JSON íŒŒì¼ì—ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "                json_data = json.loads(content_bytes.decode('utf-8'))\n",
        "                if not isinstance(json_data, list): raise ValueError(\"JSON ë°ì´í„°ëŠ” ê°ì²´ ëª©ë¡ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\")\n",
        "                if json_data and isinstance(json_data[0], dict):\n",
        "                    common_keys = ['content', 'text', 'body', 'document', 'description']\n",
        "                    found_key = next((key for key in common_keys if key in json_data[0]), None)\n",
        "                    if found_key:\n",
        "                        json_content_key_input.value = found_key\n",
        "                        print(f\"â„¹ï¸ ì½˜í…ì¸  í‚¤ë¡œ '{found_key}'ë¥¼ ìë™ ê°ì§€í–ˆìŠµë‹ˆë‹¤.\")\n",
        "                content_key = json_content_key_input.value\n",
        "                for i, item in enumerate(json_data):\n",
        "                    if not isinstance(item, dict): continue\n",
        "                    metadata = item.get('metadata', {})\n",
        "                    content = item.get(content_key, \"\")\n",
        "                    meta_str = \", \".join([f\"{k}: {v}\" for k, v in metadata.items()])\n",
        "                    combined_text = f\"ë©”íƒ€ë°ì´í„°: [ {meta_str} ]\\në‚´ìš©: {content}\"\n",
        "                    new_docs.append({\"id\": i, \"text\": combined_text, \"metadata\": metadata})\n",
        "            else:\n",
        "                full_text = content_bytes.decode('utf-8')\n",
        "                if method == 'chunk':\n",
        "                    chunks = split_text_into_chunks(full_text, chunk_size_input.value, chunk_overlap_input.value)\n",
        "                    new_docs = [{\"id\": i, \"text\": chunk, \"metadata\": {}} for i, chunk in enumerate(chunks)]\n",
        "                else:\n",
        "                    lines = full_text.splitlines()\n",
        "                    new_docs = [{\"id\": i, \"text\": line.strip(), \"metadata\": {}} for i, line in enumerate(lines) if line.strip()]\n",
        "            update_documents(new_docs)\n",
        "        except Exception as e:\n",
        "            with output_area: print(f\"âŒ íŒŒì¼ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
        "            with download_area: clear_output()\n",
        "\n",
        "def on_search_button_clicked(b):\n",
        "    \"\"\"â˜…ë³µì›â˜…: ê²€ìƒ‰ ë²„íŠ¼ì„ í´ë¦­í–ˆì„ ë•Œì˜ ë©”ì¸ ì²˜ë¦¬\"\"\"\n",
        "    global documents, doc_embeddings, last_embedding_model, last_division_settings, current_query\n",
        "\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        current_query = query_input.value\n",
        "        embedding_model = embedding_model_selector.value\n",
        "        current_division_settings = (division_method_selector.value, chunk_size_input.value, chunk_overlap_input.value, json_content_key_input.value)\n",
        "\n",
        "        if not current_query: print(\"âŒ ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤.\"); return\n",
        "        if not documents: print(\"âŒ ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.\"); return\n",
        "\n",
        "        print(f\"ğŸ” ê²€ìƒ‰ ì‹œì‘...\\nì¿¼ë¦¬: {current_query}\\ní¬í•¨ëœ ëª¨ë¸: {embedding_model}\\n\" + \"-\" * 30)\n",
        "\n",
        "        results = []\n",
        "        doc_texts = [d['text'] for d in documents]\n",
        "\n",
        "        if embedding_model == 'none':\n",
        "            print(\"í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘...\")\n",
        "            query_words = current_query.lower().split()\n",
        "            for doc_info in documents:\n",
        "                score = sum(1 for word in query_words if word in doc_info['text'].lower())\n",
        "                results.append({'doc_info': doc_info, 'score': score})\n",
        "            results.sort(key=lambda x: x['score'], reverse=True)\n",
        "        else:\n",
        "            if doc_embeddings is None or last_embedding_model != embedding_model or last_division_settings != current_division_settings:\n",
        "                print(f\"ë¬¸ì„œ í¬í•¨ì„ ìƒì„±í•˜ëŠ” ì¤‘ (ëª¨ë¸: {embedding_model})...\")\n",
        "                temp_embeddings = [get_embedding(text, embedding_model) for text in doc_texts]\n",
        "\n",
        "                valid_docs_with_embeddings = [(documents[i], emb) for i, emb in enumerate(temp_embeddings) if emb is not None]\n",
        "                if not valid_docs_with_embeddings: print(\"âŒ ëª¨ë“  ë¬¸ì„œì˜ í¬í•¨ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\"); return\n",
        "\n",
        "                valid_docs, doc_embeddings_list = zip(*valid_docs_with_embeddings)\n",
        "                documents = list(valid_docs)\n",
        "                doc_embeddings = np.array(doc_embeddings_list)\n",
        "                last_embedding_model = embedding_model\n",
        "                last_division_settings = current_division_settings\n",
        "                print(\"âœ… í¬í•¨ ì™„ë£Œ.\")\n",
        "\n",
        "            print(\"ê²€ìƒ‰ì–´ í¼ê°€ê¸° ìƒì„± ì¤‘...\")\n",
        "            query_embedding = get_embedding(current_query, embedding_model)\n",
        "            if query_embedding is None: print(\"âŒ ì¿¼ë¦¬ë¥¼ ì‚½ì…í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\"); return\n",
        "\n",
        "            print(\"ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\")\n",
        "            similarities = [1 - cosine(query_embedding, doc_emb) for doc_emb in doc_embeddings]\n",
        "            sorted_indices = np.argsort(similarities)[::-1]\n",
        "            results = [{'doc_info': documents[i], 'score': similarities[i]} for i in sorted_indices]\n",
        "\n",
        "        print(f\"\\nğŸ† ê²€ìƒ‰ ê²°ê³¼ (ì „ì²´{len(results)}ê±´) ğŸ†\\n\")\n",
        "        if not results:\n",
        "            print(\"ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        else:\n",
        "            for i, result in enumerate(results):\n",
        "                score_str = f\"{result['score']:.4f}\" if isinstance(result['score'], float) else str(result['score'])\n",
        "                print(f\"ã€Rank {i+1}ã€‘(ë¶„í• ID: {result['doc_info']['id']}) Score: {score_str}\")\n",
        "                print(f\"   ğŸ“„ {result['doc_info']['text']}\")\n",
        "                print(\"-\" * 20)\n",
        "\n",
        "    display_results_downloader(results, current_query)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 7. UIì˜ ìµœì¢… ì¡°ë¦½ ë° ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ë“±ë¡\n",
        "# ------------------------------------------------\n",
        "division_method_selector.observe(on_division_method_change, names='value')\n",
        "file_uploader.observe(on_file_upload, names='value')\n",
        "search_button.on_click(on_search_button_clicked)\n",
        "on_division_method_change({'new': division_method_selector.value})\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    widgets.HTML(\"<h2>ê²€ìƒ‰ ìˆœìœ„ í‰ê°€ ë„êµ¬</h2>\"),\n",
        "    widgets.HTML(\"\"\"\n",
        "    <p><b>ì‚¬ìš©ë²•:</b></p>\n",
        "    <ol>\n",
        "      <li><b>ë¶„í•  ë°©ë²•</b>ì„ ì„ íƒí•©ë‹ˆë‹¤.(JSONì˜ ê²½ìš° 'JSON ê°ì²´ë³„'ì„ ì„ íƒí•˜ë©´ í¸ë¦¬í•©ë‹ˆë‹¤.)</li>\n",
        "      <li><b>íŒŒì¼ ì—…ë¡œë“œ</b>: JSONì˜ ê²½ìš° ì½˜í…ì¸  í‚¤ê°€ ìë™ìœ¼ë¡œ ì¶”ì¸¡ë©ë‹ˆë‹¤.</li>\n",
        "      <li>(ì„ íƒ ì‚¬í•­) ë¶„í• ëœ ëª¨ë“  ë¬¸ì„œë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì˜ë„í•œ ëŒ€ë¡œ í™•ì¸í•©ë‹ˆë‹¤.</li>\n",
        "      <li><b>ê²€ìƒ‰ì–´ ì…ë ¥</b>ì„ í†µí•´ 'ê²€ìƒ‰ ì‹¤í–‰'ì„ ìˆ˜í–‰í•˜ë©´ ì „ì²´ ìˆœìœ„ê°€ í‘œì‹œë˜ë©° CSVì—ì„œ ê²°ê³¼ë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>\n",
        "    </ol>\n",
        "    \"\"\"),\n",
        "    widgets.HBox([file_uploader, division_method_selector]),\n",
        "    division_settings_box,\n",
        "    embedding_model_selector,\n",
        "    widgets.HBox([query_input, search_button]),\n",
        "    widgets.HTML(\"<hr>\"),\n",
        "    output_area,\n",
        "    download_area\n",
        "])\n",
        "\n",
        "display(ui)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
